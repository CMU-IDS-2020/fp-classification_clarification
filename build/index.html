<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, minimum-scale=1.0"
    />
    <title>Final Project</title>
    <meta property="og:title" content="Final Project" />
    <meta charset="utf-8" />
    <meta property="og:type" content="article" />

    <meta property="og:description" content="Short description of your project" />
    <meta property="description" content="Short description of your project" />
    <link
      rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css"
    />

    <link rel="stylesheet" href="static/idyll_styles.css" />
  </head>
  <body>
    <div id="idyll-mount"><div data-reactroot=""><div class="idyll-root"><div class=" idyll-text-container"></div><div class="article-header idyll-header"><h1 class="hed"><span class="beginners-guide">A Beginners Guide To:</span><br/>Machine Learning Classification</h1><div class="byline">By: <a href="https://www.linkedin.com/in/laura-m-howard/">Laura Howard</a>, <a href="https://www.linkedin.com/in/christian-deverall-cmu/">Christian Deverall</a>, <a href="https://www.linkedin.com/in/nathanjen/">Nathan Jen</a>, and <a href="https://www.linkedin.com/in/juliettenwong/">Juliette Wong</a></div><div class="scrollContainer"><p>Scroll to Learn!</p></div></div><div class="idyll-scroll" id="idyll-scroll-0" style="position:relative"><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><h1 id="a-brief-history-of-classification">A Brief History of Classification</h1><span class="aside-container"><span class="aside"><img src="static/images/machine_learning_cartoon.png"/></span></span><p class="introSubtitle">Can computers learn from data?</p><p>This question led to the development of machine learning.  In 1959, Arthur Samuel (a pioneer computer scientist) 
    defined machine learning as “the field of study that <b> gives computers the ability to learn without being explicitly programmed</b>”.</p><p>While this may sound complex, we hope to make some machine learning concepts approachable and easy to learn 
    through examples and activities in this article.
  </p></div><div class="idyll-step "><p class="introSubtitle">Understanding Algorithms</p><p><span class="aside-container"><span class="aside"><img src="static/images/ml_books.png"/></span></span>
    
    Think of it this way – as you can learn these concepts through experimenting with examples, computers can do the same!</p><p>To grasp how machine learning works, we need to understand <b>algorithms - the method for creating models from data</b>.  
    In this article, we will show you popular algorithms that perform classification tasks on a simple dataset related to income.  
  </p></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-1" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><img src="static/images/students.png"/></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><h1 id="case-study">Case Study</h1><p><p class="introSubtitle">Is Income Predictable?</p>
    
    You are starting your senior year of high school, and are <b>trying to decide if you will further your education beyond graduation</b>.
    You are wondering if you will see high enough returns in the long run that will allow you to live a comfortable life and pay off student loans.
    To do this, you look at US census data, and gather real examples of peoples:  </p><p class="numbered-list">1. Age <br/>2. Number of years of education <br/>3. What their income is <br/></p><p>You classify income as high if it exceeds $50,000 (Data is from the 1994 US census bureau database. $50,000 is worth ~$88,000 in 2020).</p><p>Lets take a quick look at the data below: </p><div class="ReactTable table "><div class="rt-table" role="grid"><div class="rt-thead -header" style="min-width:300px"><div class="rt-tr" role="row"><div class="rt-th rt-resizable-header -cursor-pointer" role="columnheader" tabindex="-1" style="flex:100 0 auto;width:100px"><div class="rt-resizable-header-content">Age</div><div class="rt-resizer"></div></div><div class="rt-th rt-resizable-header -cursor-pointer" role="columnheader" tabindex="-1" style="flex:100 0 auto;width:100px"><div class="rt-resizable-header-content">Years of Education</div><div class="rt-resizer"></div></div><div class="rt-th rt-resizable-header -cursor-pointer" role="columnheader" tabindex="-1" style="flex:100 0 auto;width:100px"><div class="rt-resizable-header-content">High Income</div><div class="rt-resizer"></div></div></div></div><div class="rt-tbody" style="min-width:300px"><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -odd" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">38</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">8</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">False</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -even" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">59</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">13</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">True</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -odd" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">26</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">13</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">False</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -even" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">51</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">13</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">True</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -odd" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">34</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">11</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">False</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -even" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">50</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">15</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">True</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -odd" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">45</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">13</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">True</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -even" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">57</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">10</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">True</div></div></div></div></div><div class="pagination-bottom"><div class="-pagination"><div class="-previous"><button type="button" disabled="" class="-btn">Previous</button></div><div class="-center"><span class="-pageInfo">Page<!-- --> <span class="-currentPage">1</span> <!-- -->of<!-- --> <span class="-totalPages">4</span></span></div><div class="-next"><button type="button" class="-btn">Next</button></div></div></div><div class="-loading"><div class="-loading-inner">Loading...</div></div></div></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-2" style="position:relative"><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><p class="introSubtitle">Exploring the dataset</p><p>As you probably realized, <b> looking at raw data isn’t very useful.</b> So now lets look at visualizations to see what
    more we can learn from this dataset!</p><div></div><p>First, let’s look at a simple bar chart showing the number of people with high income and without. 
    As you can see here, there is an equal number of people (16) with high and low income in the dataset, 
    totaling 32 records.</p><span class="aside-container"><span class="aside">
      Color this chart by:
      <button>High Income</button><button>None</button></span></span><div></div><p>Next, let’s look at the spread of peoples’ ages. When the high income button is selected to the right, you can see how old 
    people who have a high income are.</p><div style="display:none">
      As you can see, it seems as if <b>older people are more likely to be high income</b>. This makes 
      sense as more experience usually means a better salary!</div><span class="aside-container"><span class="aside">
      Color this chart by:
      <button>High Income</button><button>None</button></span></span><div></div><p>This graph shows the amount of people by years of education. When the high income button is selected, you 
    can see how many years of education people with a high income have.</p><div style="display:none">
      As you can see from this graph, those with <b>high income tend to be the ones with more years of education</b>.
      However, <b>this is not always the case</b>, as you will see some low income individuals 
      with 9, 10, 11, and 13 years of experience. 
    </div><div></div><p>Finally, let’s look at the age and education variables together in a scatterplot.</p><p>After looking at the data, do you think more years of schooling truly result in a higher income for you?
  </p></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-3" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><img src="static/images/mystery-person.png"/></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><p><p class="introSubtitle">Evaluating Your Model</p>
    After exploring the data, we hope that you got a sense of which patterns are more likely
    to mean that someone is high income or not. By doing this, <b>you were actually training your own model</b>, which is 
    almost exactly what machine learning models do! However, now that your model is trained, lets put it to the test
    and have you predict the income of 3 mystery people.</p><div class="person-section"><p><b>Mystery Person #1:</b> <br/>
      Age: 57 <br/>
      Years of Education: 15</p><div class="btn-row" idyll="[object Object]"><button class="evaluate-btn">High Income</button><button class="evaluate-btn">Low Income</button></div><div></div></div><div class="person-section"><p><b>Mystery Person #2:</b> <br/>
      Age: 22 <br/>
      Years of Education: 7</p><div class="btn-row" idyll="[object Object]"><button>High Income</button><button>Low Income</button></div><div></div></div><div class="person-section"><p><b>Mystery Person #3:</b> <br/>
      Age: 30 <br/>
      Years of Education: 10</p><div class="btn-row" idyll="[object Object]"><button class="evaluate-btn">High Income</button><button class="evaluate-btn">Low Income</button></div><div></div></div></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-4" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><img src="static/images/training.png"/></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><h1 id="using-machine-learning">Using Machine Learning</h1><p>We will now complete the same classifying task with three commonly used classification algorithms:  </p><p><a href="#knn">1. K Nearest Neighbors (KNN)</a>  </p><p><a href="#decisionTrees">2. Decision Trees</a>  </p><p><a href="#logreg">3. Logistic Regression</a>  </p></div><div class="idyll-step "><p><p class="introSubtitle">Training</p>
    
    Before we get into the algorithms, we need to understand why models need training. 
    Training a model simply means teaching the model good values for a set of examples.  
    Each example will help the model to understand the relationship between the variables 
    (in this case, age and education years) and the classification value (income). 
    Once the relationship is understood, the model will be able to predict an unknown classification value for new sets of variables.</p></div><div class="idyll-step "><img src="static/images/warning3.gif"/><p class="introSubtitle">Warning: Overfitting</p><p>One thing to pay attention to when using machine learning algorithms is overfitting.  
    Overfitting happens when a model is too complex and starts to classify according 
    to a random error in the data over the expected relationships between variables.  
    A model is considered <b>“overfit” when it fits your training data really well, yet performs 
    poorly on new data</b>. </p><p>One way to identify an “overfit” is to reserve a portion of your data 
    set and introduce it after you are finished creating your model to see how it performs. If you model
    performs poorly on the reserved set, it is overfit to the training data!
    
    A way to understand how the model performs is by adjusting the hyperparameters, which are higher-level properties of the model. 
    In our case, they would be the value of K, the depth of the decision tree, or the threshold value for the logistical regression
      <img src="static/images/under_over.png"/></p></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-5" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><div><img src="static/images/knn3.png"/></div><div style="display:none"><img src="static/images/knn6.png"/></div></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step " id="knn"><h1 id="k-nearest-neighbors-(knn)">K Nearest Neighbors (KNN)</h1><p>The K Nearest Neighbors (KNN) algorithm <b>classifies data based on data that is most similar</b>.  
    KNN uses similar data for classification by plotting a test point with training data points and classifies
    the test point based on the class of the number (K) of points closest to the test point.
  </p></div><div class="idyll-step "><p>
    In this example the the right, our “test point” is the red point. Click on the buttons to see how different values of k can lead to different classifications.</p><div class="btn-row"><button>k = 3</button><button>k = 6</button></div><div>
    When we set K = 3, we see that there are two points near it that are blue, and one that is black.  
    Since we would <b>classify based on the majority, we would classify our test point as blue</b>.
    </div><div style="display:none">
    However, when we set K = 6, four of the nearest neighbors are black, while only two are blue. 
    Thus, when K = 6, we would <b>label our test point as black since black now has the majority</b>.
    </div></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-6" style="position:relative"><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><p>Here, we will apply KNN to our dataset, where you can interact and vary the number of neighbors (K) to see how it will 
    affect what the model predicts.
    
    <div id="vis"></div><br/>
    Here, you can play around with the number of neighbors (k) to see what our model would predict 
    for the last mystery person with an age of 30 and 10 years of education.  
    If you remember from above, this person is classified as low income, but if you make K is 2 or less, 
    the KNN algorithm would actually predict high income. This is because the <b>nearest neighbor is classified as high income</b>. </p><p>However, having K = 2 presents an interesting problem as the neighbors are classified differently. While there are many approches to break
    a tie, we are <b>using the “nearest” approach, which uses the class with the nearest neighbor</b>. There is also the “random” approach, which
    selects the class of a random neighbor.
  </p></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-7" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><img src="static/images/dt.png"/></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step " id="decisionTrees"><h1 id="decision-trees">Decision Trees</h1><p>The decision tree is an important machine learning algorithm that is commonly used for classification problems, just this case!
    This algorithm <b>follows a set of if-else conditions to classify data according to the conditions</b>. An example of a decision tree is
    shown on the right. 
    
    <button>
      Click here to learn about how to train decision trees!</button><div style="display:none"><p>
      On a high level, decision trees work by <b>reducing information entropy</b> as the data moves down the tree and answers a set of questions.</p><p><b>Information entropy can be thought of as a measurement of uncertainty</b>. Analogously, it can be thought of as how surprised you would 
      be to discover the real label. If all you know is that half of the labels are true and the other half are false, your uncertainty 
      when making predictions would be very high. On the other extreme, if all labels are true then you would not be surprised at all 
      upon discovering the real label. Your uncertainty would be very low and thus entropy would be 0. </p><p><b>At each step (also called a “split”), there is a question about the data at each node in the tree</b>. </p><p>The format of a split is “which data points have a value of less than V for the attribute A”.</p><p>1. Attributes that contain any numerical value within a range are called continuous. Both age and years of education are therefore considered continuous. 
      The important first step in the algorithm is to create discretized values for each attribute. To keep things simple, age is discretized in steps of 5 
      (20, 25,30..) and the years of education are discretized in steps of 2 (6, 8, 10...). Each discretization forms a candidate for a potential split.
      <br/><br/>2. You should calculate the entropy of the data at the current node using the formula for Shannon information entropy as shown on the right. 
      This gives an indication of how mixed the dataset is at this level and the goal is to create a split that reduces the entropy of our child nodes by the most.
      <br/><br/>3. For each discretization and for each attribute:
              <br/><br/>3a. Data points that have a value less than V for attribute A are passed to the left node while data that have a value greater or equal to V for attribute 
            A are passed to the right node.
              <br/><br/>3b. Take a weighted average of the entropy of both child nodes where the weightings are equal to the number of data points on the child nodes divided by 
            the number of data points on the parent nodes. This average of child entropies is also called “the conditional entropy”.
              <br/><br/>3c. Subtract the parent entropy by this weighted average in order to calculate the information gain.
            Choose the split that results in the highest information gain. To avoid making excessive splits, make sure to stop if the highest information gain is 0.
              <br/><br/>3d. Iteratively repeat steps 2 and 3 until you have reached the max depth or until all the labels of the data points belong to the same class (remember 
            that this is when entropy equals 0).
    </p></div></p></div><div class="idyll-step "><p><p class="introSubtitle">What is depth?</p>
    The <b>depth of decision trees refers to the number of layers that the tree has</b>.  Decision tree depth is a delicate balance, as <b>too much depth could cause overfitting, 
    and too little depth could lead to less accuracy</b>.  To ensure that your tree has appropriate depth, there are two commonly used methods:  
    <br/><br/>1. Grow the tree until it overfits, then start cutting back  
    <br/><br/>2. Prevent the tree from growing by stopping it just before perfect classification of the training data
    Let’s look at how the data performs at different depths below.
  </p></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-8" style="position:relative"><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><p>
    Below is a visualization showing the decision tree run on our census dataset! Use the scroller to change the maximum depth of 
    the decision tree.
    </p><div><p class="introSubtitle">Depth 1</p><img src="static/images/depth1.png" style="width:40%;margin-left:0" idyll="[object Object]"/></div><div style="display:none"><p class="introSubtitle">Depth 2</p><img src="static/images/depth2.png" style="width:65%;margin-left:0" idyll="[object Object]"/></div><div style="display:none"><p class="introSubtitle">Depth 3</p><img src="static/images/depth3.png" style="width:82%;margin-left:0" idyll="[object Object]"/></div><p><div style="display:none"><p class="introSubtitle">Depth 4</p><img src="static/images/depth4.png"/></div>1 <input type="range" value="1" min="1" max="4" step="1"/> 4</p></div><div class="idyll-step "><p><div id="dt"></div><br/><br/>
    Now, let’s gain a different perspective on how the decision tree makes predictions. <b>Each line in the graph above represents a split 
    in the decision tree</b>. This graph shows how the decision tree breaks down the data into smaller and smaller rectangles and makes the 
    prediction based on which rectangle the data point lands in. <br/></p><p>One interesting note thing to note is that <b>as the max depth increases, the decision tree tries to account for the anomalous data point </b>
    (the 32 year old with 9 years of education who is a high earner). If we were to increase max depth past 4, the decision tree would wrap
    around the square containing the anomalous data point and predict all future data points within that square as “High Income”. 
    What is this an example of? <br/></p><div idyll="[object Object]"><button class="evaluate-btn">Overfitting</button><button class="evaluate-btn">Underfitting</button></div><div></div></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-9" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><div><div style="height:100%;width:100%;user-select:none;pointer-events:none;touch-action:none;position:relative" class="VictoryContainer"><svg width="450" height="300" role="img" aria-labelledby="victory-container-1-title victory-container-1-desc" viewBox="0 0 450 300" style="pointer-events:all;width:100%;height:100%"><g style="height:100%;width:100%;user-select:none"><defs><clipPath id="victory-clip-2"><rect x="50" y="50" width="350" height="200"></rect></clipPath></defs><g clip-path="url(#victory-clip-2)"><path style="fill:transparent;stroke:#252525;stroke-width:2" shape-rendering="auto" role="presentation" d="M50,199.99999979388463L50.349999999999994,199.9999997854729L50.69999999999999,199.9999997767179L51.05000000000001,199.99999976760557L51.4,199.9999997581214L51.74999999999999,199.99999974825013L52.09999999999999,199.99999973797603L52.45000000000001,199.9999997272826L52.800000000000004,199.99999971615281L53.14999999999999,199.99999970456878L53.499999999999986,199.999999692512L53.85000000000001,199.9999996799632L54.2,199.99999966690223L54.55,199.99999965330827L54.89999999999999,199.9999996391595L55.250000000000014,199.99999962443331L55.60000000000001,199.99999960910617L55.949999999999996,199.99999959315346L56.29999999999999,199.99999957654975L56.65000000000001,199.99999955926842L57.00000000000001,199.99999954128182L57.35,199.99999952256118L57.69999999999999,199.99999950307654L58.05000000000001,199.9999994827967L58.400000000000006,199.99999946168924L58.75,199.99999943972034L59.099999999999994,199.9999994168549L59.44999999999999,199.99999939305633L59.80000000000001,199.99999936828647L60.150000000000006,199.99999934250576L60.49999999999999,199.9999993156729L60.84999999999999,199.999999287745L61.20000000000001,199.9999992586773L61.550000000000004,199.99999922842335L61.9,199.9999991969347L62.249999999999986,199.999999164161L62.60000000000001,199.99999913004976L62.95,199.9999990945464L63.3,199.99999905759415L63.64999999999999,199.99999901913384L64.00000000000001,199.99999897910394L64.35000000000001,199.99999893744038L64.7,199.9999988940765L65.04999999999998,199.9999988489429L65.4,199.99999880196737L65.75,199.99999875307475L66.1,199.99999870218676L66.44999999999999,199.999998649222L66.80000000000001,199.9999985940957L67.15,199.99999853671966L67.5,199.99999847700207L67.85,199.9999984148473L68.19999999999999,199.99999835015603L68.55000000000001,199.9999982828246L68.9,199.99999821274537L69.25,199.9999981398061L69.60000000000002,199.99999806389016L69.95000000000002,199.99999798487602L70.30000000000001,199.99999790263723L70.64999999999999,199.99999781704224L70.99999999999999,199.99999772795405L71.35000000000001,199.9999976352301L71.7,199.99999753872203L72.05,199.99999743827536L72.39999999999999,199.9999973337294L72.75000000000001,199.99999722491685L73.10000000000001,199.99999711166353L73.44999999999999,199.9999969937883L73.8,199.9999968711025L74.15,199.99999674340975L74.5,199.99999661050578L74.85,199.9999964721779L75.19999999999999,199.99999632820476L75.55000000000001,199.99999617835596L75.9,199.9999960223917L76.25,199.99999586006246L76.6,199.9999956911084L76.94999999999999,199.9999955152592L77.30000000000001,199.99999533223348L77.65,199.9999951417383L78,199.9999949434689L78.35000000000002,199.99999473710795L78.70000000000002,199.9999945223253L79.05,199.99999429877715L79.4,199.99999406610584L79.74999999999999,199.99999382393906L80.10000000000001,199.99999357188923L80.45,199.99999330955308L80.8,199.99999303651077L81.14999999999999,199.9999927523254L81.50000000000001,199.9999924565422L81.85000000000001,199.99999214868788L82.2,199.9999918282698L82.54999999999998,199.9999914947752L82.9,199.9999911476704L83.25,199.99999078640002L83.6,199.9999904103859L83.94999999999999,199.99999001902637L84.30000000000001,199.99998961169516L84.65,199.99998918774043L85,199.99998874648378L85.35,199.99998828721914L85.7,199.99998780921155L86.05000000000001,199.9999873116961L86.4,199.99998679387664L86.75,199.9999862549246L87.1,199.99998569397752L87.45000000000002,199.99998511013774L87.80000000000001,199.99998450247102L88.14999999999999,199.999983870005L88.5,199.9999832117275L88.85000000000001,199.99998252658526L89.2,199.99998181348178L89.55,199.99998107127604L89.9,199.99998029878032L90.25000000000001,199.99997949475846L90.60000000000001,199.99997865792386L90.94999999999999,199.9999777869374L91.29999999999998,199.9999768804053L91.65,199.99997593687698L92,199.99997495484254L92.35,199.99997393273054L92.69999999999999,199.9999728689054L93.05,199.99997176166474L93.4,199.99997060923675L93.75,199.99996940977732L94.1,199.99996816136704L94.45,199.9999668620082L94.80000000000001,199.99996550962157L95.15,199.999964102043L95.5,199.99996263702016L95.85,199.9999611122086L96.20000000000002,199.99995952516838L96.55000000000001,199.99995787335988L96.89999999999999,199.99995615413982L97.25,199.99995436475714L97.60000000000002,199.99995250234844L97.95,199.99995056393345L98.3,199.99994854641037L98.65,199.99994644655064L99.00000000000001,199.99994426099414L99.35000000000001,199.99994198624347L99.69999999999999,199.9999396186586L100.04999999999998,199.99993715445083L100.4,199.99993458967703L100.75,199.9999319202329L101.1,199.99992914184688L101.44999999999999,199.99992625007292L101.8,199.9999232402836L102.15,199.99992010766266L102.5,199.99991684719723L102.85,199.9999134536699L103.19999999999999,199.99990992165036L103.55000000000001,199.9999062454866L103.9,199.9999024192961L104.25,199.99989843695604L104.6,199.99989429209398L104.95000000000002,199.99988997807725L105.3,199.99988548800255L105.64999999999999,199.99988081468484L106,199.9998759506459L106.35000000000001,199.99987088810224L106.7,199.9998656189528L107.04999999999998,199.99986013476592L107.4,199.99985442676575L107.75000000000001,199.99984848581835L108.1,199.99984230241705L108.44999999999999,199.99983586666715L108.8,199.99982916827022L109.14999999999999,199.99982219650747L109.5,199.9998149402227L109.85,199.99980738780448L110.19999999999999,199.99979952716743L110.55,199.999791345733L110.9,199.9997828304093L111.25,199.9997739675702L111.6,199.99976474303347L111.94999999999999,199.99975514203803L112.30000000000001,199.9997451492205L112.65,199.99973474859047L113,199.99972392350497L113.35,199.99971265664186L113.70000000000002,199.99970092997202L114.05,199.99968872473065L114.4,199.99967602138707L114.75,199.99966279961365L115.10000000000001,199.9996490382531L115.45,199.99963471528483L115.8,199.9996198077895L116.15,199.99960429191242L116.50000000000001,199.99958814282553L116.85,199.9995713346874L117.2,199.9995538406022L117.55,199.9995356325763L117.9,199.99951668147384L118.25,199.99949695696984L118.60000000000001,199.9994764275018L118.94999999999999,199.99945506021925L119.3,199.99943282093102L119.65,199.99940967405075L120,199.9993855825398L120.35000000000001,199.9993605078479L120.7,199.9993344098518L121.05000000000001,199.99930724679072L121.4,199.9992789751998L121.75,199.99924954984027L122.10000000000001,199.9992189236275L122.45,199.99918704755515L122.80000000000001,199.99915387061722L123.14999999999999,199.99911933972618L123.5,199.99908339962803L123.85,199.99904599281408L124.2,199.99900705942883L124.55,199.99896653717417L124.9,199.99892436120982L125.25,199.99888046404948L125.60000000000001,199.998834775453L125.95,199.99878722231384L126.30000000000001,199.9987377285423L126.65,199.99868621494366L127.00000000000001,199.99863259909154L127.35,199.99857679519604L127.70000000000002,199.99851871396652L128.05,199.99845826246872L128.4,199.9983953439761L128.75,199.99832985781518L129.10000000000002,199.99826169920445L129.45,199.9981907590867L129.8,199.9981169239547L130.15,199.99804007566965L130.5,199.997960091272L130.85000000000002,199.99787684278508L131.2,199.99779019701015L131.55,199.9977000153135L131.89999999999998,199.9976061534048L132.25,199.99750846110607L132.6,199.99740678211168L132.95,199.99730095373846L133.3,199.9971908066653L133.65,199.99707616466247L134,199.99695684430995L134.35000000000002,199.9968326547039L134.7,199.99670339715146L135.05,199.99656886485315L135.39999999999998,199.99642884257204L135.75,199.9962831062897L136.1,199.996131422848L136.45,199.99597354957638L136.8,199.9958092339037L137.15,199.99563821295476L137.5,199.99546021312977L137.85,199.99527494966713L138.2,199.9950821261882L138.55,199.99488143422366L138.9,199.9946725527202L139.25,199.99445514752773L139.60000000000002,199.99422887086502L139.95,199.99399336076414L140.3,199.99374824049193L140.65,199.99349311794793L141,199.99322758503803L141.35000000000002,199.99295121702224L141.7,199.99266357183623L142.05,199.99236418938506L142.4,199.99205259080827L142.75,199.99172827771483L143.10000000000002,199.99139073138724L143.45,199.99103941195304L143.8,199.99067375752261L144.15,199.99029318329204L144.5,199.98989708060924L144.85000000000002,199.98948481600223L145.2,199.98905573016796L145.55,199.98860913691976L145.9,199.9881443220922L146.25,199.9876605424014L146.60000000000002,199.9871570242587L146.95,199.98663296253665L147.3,199.98608751928435L147.65,199.9855198223908L148,199.984928964194L148.35000000000002,199.98431400003332L148.7,199.98367394674318L149.05,199.98300778108586L149.39999999999998,199.98231443812L149.75,199.98159280950367L150.1,199.9808417417279L150.45000000000002,199.9800600342789L150.8,199.97924643772575L151.15,199.97839965173034L151.5,199.97751832297666L151.85000000000002,199.9766010430164L152.2,199.97564634602682L152.55,199.97465270647837L152.89999999999998,199.97361853670716L153.25,199.97254218438985L153.6,199.9714219299158L153.95000000000002,199.97025598365317L154.3,199.96904248310446L154.65000000000003,199.96777948994685L155,199.96646498695335L155.35000000000002,199.965096874789L155.7,199.9636729686782L156.05,199.9621909949373L156.39999999999998,199.96064858736736L156.75,199.9590432835014L157.1,199.95737252070052L157.45000000000002,199.95563363209257L157.8,199.9538238423476L158.15,199.95194026328298L158.5,199.94997988929202L158.85,199.9479395925889L159.2,199.9458161182623L159.55,199.94360607913077L159.9,199.94130595039192L160.25,199.93891206405655L160.6,199.93642060316049L160.95,199.9338275957443L161.3,199.93112890859265L161.65,199.92832024072337L162,199.92539711661632L162.35000000000002,199.92235487917293L162.7,199.91918868239438L163.05,199.9158934837691L163.4,199.91246403635708L163.75,199.90889488055996L164.10000000000002,199.90518033556464L164.45,199.9013144904479L164.8,199.89729119492884L165.15,199.8931040497558L165.5,199.88874639671397L165.85000000000002,199.88421130823903L166.2,199.87949157662212L166.55,199.8745797027903L166.89999999999998,199.86946788464732L167.25,199.86414800495712L167.6,199.85861161875394L167.95,199.85284994026105L168.29999999999998,199.84685382929968L168.65,199.8406137771697L169,199.83411989198254L169.35000000000002,199.82736188342628L169.7,199.82032904694216L170.05,199.8130102472916L170.39999999999998,199.80539390149144L170.75,199.797467961095L171.1,199.7892198937957L171.45000000000002,199.7806366643291L171.8,199.77170471464967L172.15,199.7624099433555L172.5,199.75273768433652L172.85000000000002,199.74267268461895L173.2,199.7321990813787L173.55,199.72130037809578L173.89999999999998,199.7099594198216L174.25,199.69815836752915L174.6,199.68587867151706L174.95000000000002,199.67310104383614L175.3,199.65980542970829L175.64999999999998,199.64597097790534L176,199.6315760100564L176.35,199.61659798885026L176.7,199.6010134851006L177.05,199.58479814363994L177.39999999999998,199.5679266480088L177.75,199.5503726839059L178.1,199.53210890136492L178.45,199.51310687562415L178.8,199.49333706665337L179.15,199.47276877730513L179.5,199.45137011005497L179.85,199.42910792229767L180.2,199.40594778016595L180.55,199.38185391083888L180.9,199.35678915330814L181.25,199.33071490757152L181.6,199.3035910822238L181.95,199.27537604041686L182.3,199.24602654416265L182.65,199.21549769695443L183,199.18374288468402L183.35,199.15071371483558L183.7,199.11635995393922L184.05,199.08062946327118L184.4,199.04346813279085L184.75,199.00481981330955L185.1,198.96462624689082L185.45000000000002,198.92282699548582L185.8,198.8793593678157L186.15,198.83415834451668L186.5,198.78715650157258L186.85,198.73828393206614L187.20000000000002,198.6874681662897L187.55,198.6346340902649L187.9,198.57970386273087L188.25,198.52259683067268L188.6,198.46322944347244L188.95000000000002,198.40151516577976L189.3,198.33736438921184L189.65,198.27068434300946L190,198.20137900379086L190.35,198.12934900456455L190.70000000000002,198.0544915431807L191.05,197.97670029042146L191.4,197.89586529795318L191.75,197.81187290638695L192.10000000000002,197.7246056537196L192.45000000000002,197.63394218445387L192.8,197.53975715972607L193.14999999999998,197.44192116879924L193.5,197.34030064231342L193.85000000000002,197.23475776771767L194.20000000000002,197.125150407346L194.54999999999998,197.01133201963637L194.9,196.89315158403326L195.25,196.77045353015495L195.60000000000002,196.64307767185176L195.95,196.5108591468263L196.29999999999998,196.37362836253516L196.65,196.2312109491394L197,196.08342772032358L197.35,195.93009464285336L197.7,195.77102281579664L198.05,195.60601846038585L198.4,195.4348829215556L198.75,195.25741268224334L199.1,195.07339939159732L199.45000000000002,194.88262990829082L199.8,194.68488636019362L200.15,194.47994622170657L200.5,194.26758241011314L200.85000000000002,194.04756340234985L201.20000000000002,193.81965337364113L201.55,193.5836123594826L201.9,193.33919644249093L202.25000000000003,193.0861579656653L202.60000000000002,192.82424577362485L202.95,192.5532054833972L203.3,192.272779786334L203.64999999999998,191.9827087827188L204.00000000000003,191.68273035060776L204.35,191.37258055040832L204.7,191.05199406664386L205.04999999999998,190.7207046882843L205.40000000000003,190.3784458289307L205.75,190.0249510880315L206.1,189.65995485417506L206.45,189.2831929513472L206.8,188.89440332885923L207.15,188.493326795445L207.5,188.07970779778825L207.85,187.6532952434776L208.20000000000002,187.21384336809186L208.55,186.76111264579345L208.9,186.29487074245404L209.25,185.81489350995122L209.60000000000002,185.32096601986174L209.95000000000002,184.81288363433407L210.3,184.29045311145472L210.64999999999998,183.7534937419304L211.00000000000003,183.20183851339243L211.35,182.63533529809948L211.70000000000002,182.0538480592673L212.04999999999998,181.45725807070178L212.39999999999998,180.84546514385326L212.75,180.21838885585817L213.10000000000002,179.57596977159082L213.45,178.91817065222529L213.79999999999998,178.24497764231126L214.15,177.5564014269073L214.5,176.85247834990176L214.85,176.13327148429104L215.2,175.39887164489483L215.55,174.6493983337662L215.9,173.8850006084249L216.25,173.1058578630005L216.6,172.312180512439L216.95000000000002,171.50421057009893L217.3,170.68222210935676L217.65,169.84652160025388L218,168.99744811276128L218.35000000000002,168.1353733789025L218.70000000000002,167.26070170677605L219.04999999999998,166.37386974043528L219.4,165.47534606063192L219.75000000000003,164.56563062257948L220.10000000000002,163.64525402815661L220.45,162.71477663131955L220.79999999999998,161.77478747692493L221.15,160.82590307465148L221.50000000000003,159.86876601124516L221.85,158.9040434058665L222.2,157.93242521487497L222.55,156.95462239392293L222.9,155.97136492671927L223.25,154.98339973124777L223.6,153.99148845555658L223.95,152.9964051764572L224.3,151.99893401555812L224.65,150.99986668799653L225,150L225.35,149.00013331200347L225.7,148.00106598444177L226.05,147.0035948235428L226.4,146.00851154444342L226.75,145.01660026875223L227.1,144.02863507328067L227.45,143.04537760607707L227.8,142.06757478512503L228.15,141.09595659413353L228.5,140.13123398875473L228.85,139.17409692534855L229.20000000000002,138.22521252307507L229.55,137.28522336868045L229.9,136.35474597184339L230.25,135.4343693774204L230.6,134.52465393936808L230.95000000000002,133.62613025956477L231.3,132.73929829322395L231.65,131.86462662109741L232,131.00255188723872L232.35,130.15347839974612L232.70000000000002,129.31777789064327L233.05,128.49578942990098L233.4,127.687819487561L233.75,126.89414213699952L234.1,126.11499939157511L234.45000000000002,125.35060166623376L234.8,124.60112835510517L235.15,123.86672851570894L235.5,123.14752165009826L235.85000000000002,122.44359857309261L236.20000000000002,121.75502235768874L236.55,121.08182934777471L236.9,120.42403022840921L237.25,119.78161114414178L237.60000000000002,119.15453485614674L237.95000000000002,118.54274192929824L238.3,117.94615194073268L238.64999999999998,117.36466470190052L239,116.79816148660757L239.35000000000002,116.24650625806959L239.70000000000002,115.70954688854528L240.04999999999998,115.18711636566593L240.4,114.67903398013823L240.75,114.18510649004875L241.10000000000002,113.70512925754599L241.45,113.23888735420655L241.8,112.78615663190811L242.15,112.3467047565224L242.50000000000003,111.92029220221178L242.84999999999997,111.50667320455497L243.20000000000002,111.10559667114072L243.55,110.71680704865281L243.9,110.34004514582497L244.24999999999997,109.97504891196851L244.60000000000002,109.62155417106928L244.95000000000002,109.27929531171569L245.3,108.94800593335614L245.64999999999998,108.62741944959166L246.00000000000003,108.31726964939222L246.35000000000002,108.01729121728124L246.70000000000002,107.72722021366599L247.04999999999998,107.44679451660281L247.39999999999998,107.17575422637515L247.75000000000003,106.91384203433466L248.10000000000002,106.66080355750907L248.45,106.4163876405174L248.79999999999998,106.18034662635887L249.15000000000003,105.95243659765012L249.50000000000003,105.73241758988686L249.85,105.52005377829343L250.2,105.31511363980636L250.55,105.11737009170915L250.90000000000003,104.92660060840265L251.24999999999997,104.74258731775669L251.6,104.56511707844439L251.95000000000002,104.39398153961412L252.3,104.22897718420336L252.64999999999998,104.06990535714664L253,103.91657227967644L253.35000000000002,103.76878905086059L253.70000000000002,103.62637163746484L254.04999999999998,103.4891408531737L254.39999999999998,103.35692232814827L254.75000000000003,103.22954646984505L255.10000000000002,103.10684841596674L255.45,102.9886679803636L255.79999999999998,102.87484959265402L256.15,102.7652422322823L256.5,102.65969935768658L256.85,102.55807883120076L257.2,102.46024284027396L257.54999999999995,102.36605781554613L257.90000000000003,102.2753943462804L258.25,102.18812709361305L258.6,102.10413470204682L258.95,102.02329970957857L259.30000000000007,101.94550845681931L259.65,101.87065099543545L260,101.79862099620917L260.35,101.72931565699054L260.70000000000005,101.66263561078816L261.04999999999995,101.59848483422027L261.4,101.53677055652756L261.75,101.47740316932729L262.1,101.42029613726913L262.45,101.36536590973512L262.79999999999995,101.3125318337103L263.15,101.26171606793389L263.5,101.21284349842742L263.85,101.16584165548329L264.2,101.12064063218429L264.54999999999995,101.07717300451415L264.90000000000003,101.03537375310916L265.25,100.99518018669042L265.6,100.95653186720915L265.95,100.91937053672879L266.3,100.88364004606078L266.65,100.84928628516442L267,100.81625711531598L267.35,100.7845023030456L267.7,100.75397345583735L268.05,100.72462395958311L268.4,100.69640891777621L268.75,100.66928509242848L269.1,100.64321084669189L269.45,100.61814608916112L269.8,100.59405221983405L270.15,100.57089207770233L270.5,100.54862988994503L270.85,100.52723122269487L271.2,100.50666293334663L271.55,100.48689312437585L271.9,100.46789109863508L272.25,100.44962731609411L272.6,100.43207335199119L272.95000000000005,100.41520185636008L273.3,100.3989865148994L273.65,100.38340201114971L274,100.36842398994358L274.35,100.35402902209464L274.70000000000005,100.34019457029174L275.05,100.32689895616386L275.4,100.31412132848294L275.75,100.30184163247085L276.1,100.29004058017841L276.45000000000005,100.27869962190422L276.8,100.2678009186213L277.15,100.25732731538102L277.5,100.24726231566348L277.85,100.2375900566445L278.20000000000005,100.2282952853503L278.55,100.21936333567086L278.9,100.21078010620434L279.25,100.20253203890499L279.6,100.19460609850856L279.95000000000005,100.1869897527084L280.3,100.17967095305784L280.65,100.17263811657372L281,100.16588010801746L281.35,100.1593862228303L281.70000000000005,100.15314617070032L282.05,100.14715005973895L282.4,100.14138838124606L282.75,100.13585199504288L283.1,100.13053211535268L283.45000000000005,100.12542029720967L283.79999999999995,100.12050842337788L284.15,100.11578869176097L284.5,100.11125360328603L284.85,100.1068959502442L285.2,100.10270880507116L285.55,100.0986855095521L285.9,100.09481966443539L286.25,100.09110511944007L286.59999999999997,100.08753596364292L286.95000000000005,100.08410651623086L287.3,100.08081131760562L287.65,100.07764512082707L288,100.07460288338365L288.35,100.07167975927663L288.70000000000005,100.06887109140732L289.05,100.0661724042557L289.4,100.06357939683951L289.75,100.06108793594345L290.1,100.05869404960808L290.45000000000005,100.05639392086923L290.79999999999995,100.05418388173774L291.15000000000003,100.05206040741109L291.5,100.05002011070795L291.85,100.04805973671702L292.2,100.04617615765241L292.54999999999995,100.0443663679074L292.90000000000003,100.04262747929951L293.25,100.04095671649858L293.6,100.03935141263267L293.95,100.03780900506268L294.3,100.0363270313218L294.65000000000003,100.034903125211L295,100.03353501304665L295.35,100.03222051005315L295.70000000000005,100.03095751689557L296.05,100.0297440163468L296.4,100.0285780700842L296.75,100.02745781561012L297.1,100.02638146329281L297.45000000000005,100.02534729352166L297.79999999999995,100.02435365397315L298.15,100.02339895698361L298.5,100.02248167702334L298.85,100.02160034826966L299.2,100.02075356227425L299.54999999999995,100.01993996572111L299.90000000000003,100.01915825827211L300.25,100.01840719049633L300.6,100.01768556188L300.95,100.01699221891417L301.29999999999995,100.01632605325682L301.65000000000003,100.01568599996665L302,100.01507103580596L302.35,100.01448017760919L302.7,100.01391248071565L303.05000000000007,100.01336703746335L303.4,100.01284297574131L303.75,100.01233945759864L304.1,100.01185567790779L304.45000000000005,100.01139086308024L304.79999999999995,100.01094426983204L305.15,100.01051518399777L305.5,100.01010291939076L305.85,100.00970681670799L306.2,100.00932624247739L306.55,100.00896058804696L306.9,100.00860926861276L307.25000000000006,100.00827172228517L307.6,100.00794740919173L307.95,100.00763581061491L308.3,100.00733642816377L308.65000000000003,100.00704878297776L309,100.00677241496197L309.35,100.00650688205207L309.7,100.00625175950807L310.05,100.00600663923586L310.4,100.00577112913498L310.75,100.00554485247227L311.1,100.0053274472798L311.45,100.00511856577634L311.8,100.00491787381179L312.15,100.0047250503329L312.5,100.00453978687025L312.85,100.00436178704524L313.2,100.00419076609629L313.55,100.00402645042362L313.9,100.00386857715199L314.25,100.00371689371028L314.6,100.00357115742796L314.95,100.00343113514685L315.3,100.00329660284854L315.65,100.00316734529613L316,100.00304315569005L316.35,100.00292383533753L316.7,100.00280919333474L317.05,100.00269904626157L317.4,100.00259321788832L317.75,100.00249153889393L318.1,100.0023938465952L318.45,100.00229998468649L318.8,100.00220980298985L319.15000000000003,100.00212315721492L319.5,100.00203990872797L319.85,100.00195992433035L320.2,100.00188307604526L320.55,100.0018092409133L320.90000000000003,100.00173830079558L321.25,100.00167014218479L321.6,100.00160465602391L321.95,100.00154173753128L322.3,100.00148128603348L322.65000000000003,100.00142320480396L323,100.00136740090846L323.35,100.00131378505634L323.7,100.0012622714577L324.05,100.00121277768613L324.40000000000003,100.00116522454704L324.75,100.0011195359505L325.1,100.00107563879018L325.45,100.00103346282583L325.8,100.0009929405712L326.15000000000003,100.00095400718592L326.5,100.000916600372L326.85,100.00088066027382L327.2,100.00084612938275L327.55,100.00081295244482L327.90000000000003,100.0007810763725L328.25,100.0007504501597L328.6,100.00072102480021L328.95,100.00069275320928L329.3,100.0006655901482L329.65000000000003,100.00063949215209L330,100.00061441746021L330.34999999999997,100.00059032594925L330.7,100.00056717906895L331.04999999999995,100.00054493978075L331.40000000000003,100.0005235724982L331.75,100.00050304303019L332.1,100.00048331852616L332.45000000000005,100.00046436742369L332.8,100.0004461593978L333.15,100.00042866531257L333.5,100.0004118571745L333.84999999999997,100.00039570808758L334.20000000000005,100.00038019221054L334.55,100.0003652847152L334.90000000000003,100.00035096174688L335.25,100.00033720038635L335.6,100.00032397861293L335.95,100.00031127526938L336.29999999999995,100.00029907002798L336.65,100.00028734335814L337,100.00027607649506L337.35,100.00026525140956L337.70000000000005,100.0002548507795L338.05,100.000244857962L338.40000000000003,100.00023525696653L338.75,100.0002260324298L339.09999999999997,100.00021716959071L339.45,100.00020865426703L339.8,100.0002004728326L340.15000000000003,100.00019261219552L340.5,100.00018505977727L340.85,100.00017780349256L341.20000000000005,100.00017083172978L341.55,100.00016413333285L341.9,100.00015769758295L342.25,100.00015151418165L342.59999999999997,100.00014557323425L342.95000000000005,100.00013986523408L343.3,100.00013438104719L343.65000000000003,100.00012911189776L344,100.00012404935413L344.34999999999997,100.00011918531519L344.7,100.00011451199748L345.05,100.00011002192278L345.4,100.00010570790602L345.75,100.00010156304393L346.1,100.00009758070391L346.45000000000005,100.00009375451339L346.8,100.00009007834961L347.15,100.0000865463301L347.5,100.00008315280277L347.84999999999997,100.00007989233731L348.2,100.00007675971639L348.55,100.00007374992711L348.90000000000003,100.00007085815312L349.25000000000006,100.00006807976706L349.6,100.00006541032297L349.95,100.00006284554917L350.3,100.00006038134143L350.65,100.0000580137565L351,100.00005573900586L351.35,100.00005355344936L351.70000000000005,100.00005145358966L352.05,100.00004943606652L352.40000000000003,100.00004749765156L352.75,100.00004563524286L353.1,100.00004384586018L353.45,100.00004212664012L353.8,100.0000404748316L354.15,100.00003888779139L354.50000000000006,100.00003736297981L354.85,100.00003589795696L355.20000000000005,100.00003449037843L355.55,100.0000331379918L355.9,100.00003183863296L356.25,100.0000305902227L356.6,100.00002939076327L356.95,100.00002823833526L357.29999999999995,100.00002713109461L357.65000000000003,100.00002606726946L358.00000000000006,100.00002504515746L358.35,100.00002406312299L358.7,100.00002311959469L359.05,100.0000222130626L359.4,100.00002134207614L359.75,100.00002050524154L360.09999999999997,100.00001970121968L360.45000000000005,100.00001892872396L360.80000000000007,100.00001818651819L361.15,100.00001747341477L361.5,100.00001678827249L361.85,100.000016129995L362.2,100.00001549752895L362.55,100.00001488986223L362.9,100.00001430602248L363.25000000000006,100.00001374507539L363.6,100.00001320612336L363.95,100.00001268830391L364.3,100.00001219078845L364.65000000000003,100.00001171278086L365,100.00001125351622L365.35,100.00001081225957L365.7,100.00001038830484L366.04999999999995,100.00000998097363L366.40000000000003,100.00000958961408L366.75,100.00000921359998L367.1,100.00000885232959L367.45,100.00000850522483L367.8,100.0000081717302L368.15000000000003,100.00000785131212L368.5,100.00000754345777L368.84999999999997,100.0000072476746L369.20000000000005,100.00000696348923L369.55,100.00000669044692L369.90000000000003,100.00000642811077L370.25,100.00000617606096L370.6,100.00000593389416L370.95,100.00000570122288L371.29999999999995,100.0000054776747L371.65,100.00000526289205L372.00000000000006,100.00000505653111L372.35,100.00000485826172L372.7,100.00000466776652L373.05,100.0000044847408L373.40000000000003,100.00000430889159L373.75,100.00000413993754L374.09999999999997,100.00000397760829L374.45,100.00000382164401L374.79999999999995,100.00000367179524L375.15000000000003,100.00000352782209L375.5,100.00000338949422L375.85,100.00000325659025L376.20000000000005,100.0000031288975L376.55,100.0000030062117L376.9,100.00000288833647L377.25,100.00000277508317L377.59999999999997,100.0000026662706L377.95000000000005,100.00000256172464L378.3,100.00000246127797L378.65000000000003,100.0000023647699L379,100.00000227204592L379.35,100.00000218295776L379.7,100.00000209736274L380.04999999999995,100.00000201512398L380.4,100.00000193610984L380.75,100.00000186019392L381.1,100.00000178725466L381.45000000000005,100.0000017171754L381.8,100.000001649844L382.15000000000003,100.00000158515269L382.5,100.00000152299796L382.84999999999997,100.00000146328034L383.2,100.0000014059043L383.55,100.000001350778L383.90000000000003,100.00000129781324L384.25,100.00000124692528L384.6,100.0000011980326L384.95000000000005,100.00000115105712L385.3,100.00000110592347L385.65,100.00000106255965L386,100.0000010208961L386.34999999999997,100.00000098086613L386.70000000000005,100.00000094240585L387.05,100.00000090545359L387.40000000000003,100.00000086995024L387.75,100.00000083583899L388.09999999999997,100.00000080306529L388.45,100.00000077157662L388.8,100.00000074132268L389.15,100.000000712255L389.50000000000006,100.00000068432712L389.85,100.00000065749427L390.20000000000005,100.00000063171353L390.55,100.0000006069437L390.9,100.0000005831451L391.25,100.00000056027966L391.59999999999997,100.00000053831079L391.95,100.00000051720329L392.3,100.00000049692346L392.65000000000003,100.00000047743882L393.00000000000006,100.00000045871818L393.35,100.00000044073158L393.7,100.00000042345025L394.05,100.00000040684654L394.4,100.00000039089386L394.75,100.00000037556666L395.1,100.00000036084049L395.45000000000005,100.00000034669173L395.8,100.00000033309777L396.15000000000003,100.00000032003683L396.5,100.000000307488L396.85,100.00000029543122L397.2,100.00000028384719L397.55,100.00000027271739L397.9,100.00000026202397L398.25000000000006,100.00000025174987L398.6,100.00000024187864L398.95000000000005,100.00000023239443L399.3,100.00000022328211L399.65,100.00000021452709"></path></g></g><g role="presentation"><line x1="50" x2="400" y1="200" y2="200" style="stroke:#252525;fill:transparent;stroke-width:1;stroke-linecap:round;stroke-linejoin:round" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><g role="presentation"><line x1="50" x2="50" y1="250" y2="50" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="50" x2="50" y1="200" y2="201" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="11.969999999999999" x="50" y="211"><tspan x="50" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="middle">-20</tspan></text></g><g role="presentation"><line x1="137.5" x2="137.5" y1="250" y2="50" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="137.5" x2="137.5" y1="200" y2="201" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="11.969999999999999" x="137.5" y="211"><tspan x="137.5" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="middle">-10</tspan></text></g><g role="presentation"><line x1="312.5" x2="312.5" y1="250" y2="50" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="312.5" x2="312.5" y1="200" y2="201" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="11.969999999999999" x="312.5" y="211"><tspan x="312.5" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="middle">10</tspan></text></g><g role="presentation"><line x1="400" x2="400" y1="250" y2="50" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="400" x2="400" y1="200" y2="201" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="11.969999999999999" x="400" y="211"><tspan x="400" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="middle">20</tspan></text></g></g><g role="presentation"><line x1="225" x2="225" y1="50" y2="250" style="stroke:#252525;fill:transparent;stroke-width:1;stroke-linecap:round;stroke-linejoin:round" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><g role="presentation"><line x1="50" x2="400" y1="250" y2="250" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="225" x2="224" y1="250" y2="250" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="4.97" x="214" y="250"><tspan x="214" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="end">-0.5</tspan></text></g><g role="presentation"><line x1="50" x2="400" y1="150" y2="150" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="225" x2="224" y1="150" y2="150" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="4.97" x="214" y="150"><tspan x="214" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="end">0.5</tspan></text></g><g role="presentation"><line x1="50" x2="400" y1="100" y2="100" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="225" x2="224" y1="100" y2="100" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="4.97" x="214" y="100"><tspan x="214" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="end">1.0</tspan></text></g><g role="presentation"><line x1="50" x2="400" y1="50" y2="50" style="stroke:transparent;fill:transparent;pointer-events:none" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><line x1="225" x2="224" y1="50" y2="50" style="stroke:transparent;fill:transparent;size:1px" role="presentation" shape-rendering="auto" vector-effect="non-scaling-stroke"></line><text dx="0" dy="4.97" x="214" y="50"><tspan x="214" dx="0" style="fill:#252525;font-size:14px;font-family:&#x27;Gill Sans&#x27;, &#x27;Gill Sans MT&#x27;, &#x27;Ser­avek&#x27;, &#x27;Trebuchet MS&#x27;, sans-serif;stroke:transparent;letter-spacing:normal;padding:10px" text-anchor="end">1.5</tspan></text></g></g></svg><div style="z-index:99;position:absolute;top:0;left:0;width:100%;height:100%"><svg width="450" height="300" viewBox="0 0 450 300" style="overflow:visible;width:100%;height:100%"></svg></div></div></div><span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x)=\frac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.2484389999999999em;vertical-align:-0.403331em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathit mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathit mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </span></span></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step " id="logreg"><h1 id="logistic-regression">Logistic Regression</h1><p>Contrary to its name, logistic regression is not a regression algorithm, but a classification algorithm. 
    It does so by performing a <b>linear regression based on the attributes, and then uses that regression to 
    calculate the probability of whether or not the data point is in a certain class</b>.
  </p></div><div class="idyll-step "><p>
    Before understanding logistic regression, one must first understand linear regression. Suppose we have a set
    of points that are in tuples (x, y). We can visualize this set of points in a scatterplot, such as one right here:
      <div><iframe src=https://www.desmos.com/calculator/sgsjnaipwe?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></p><p>What if we wanted to fit a line that best fits this data? Our data is noisy, so it won’t fit a line perfectly, 
    but one can manually try to do so. 
      <div><iframe src=https://www.desmos.com/calculator/plfizuubiv?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></p><p>Alternatively, one can use a method called “ordinary least squares”, or OLS for short. 
    <b>Since a line can be written as a function, any line can be used to create estimates of an output given 
    input values</b>. Each point has a corresponding residual, which is computed by taking the difference between 
    the actual value and our estimated value. OLS is the method that calculates the slope and intercept of the 
    line that minimizes the sum of the squares of these residual values: <br/><br/><span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>m</mi><mo separator="true">,</mo><mi>b</mi></mrow></msub><msub><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow></msub><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mo>(</mo><mi>m</mi><mo>⋅</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>b</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">min_{m, b} \sum_{x_i \in points} (y_i - (m\cdot x_i - b))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.2499259999999999em;vertical-align:-0.43581800000000004em;"></span><span class="base"><span class="mord mathit">m</span><span class="mord mathit">i</span><span class="mord"><span class="mord mathit">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">m</span><span class="mpunct mtight">,</span><span class="mord mathit mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathit mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mathit mtight">p</span><span class="mord mathit mtight">o</span><span class="mord mathit mtight">i</span><span class="mord mathit mtight">n</span><span class="mord mathit mtight">t</span><span class="mord mathit mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mopen">(</span><span class="mord mathit">m</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit">b</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> </span></span></p><p><b>This scary equation pretty much just helps us find the line that best fits our data points.</b>
    So, for our example, our ordinary least-squares regression line is: 
    <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn><mi>x</mi><mo>+</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">Y=0.5x + 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.22222em;">Y</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord mathit">x</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord">3</span></span></span></span> </span></span><div><iframe src=https://www.desmos.com/calculator/ja7f5rvr8v?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></p></div><div class="idyll-step "><p>
    Now that we know the basics of linear regression, we can move on to logistic regression! While explaining 
    the theory behind logistic regression is beyond the scope of this article, we will explain in the overall concept of
    how to perform logistic regression. </p><p>For a given data point, we can find the OLS estimate using our least-squares regression line equation. 
    This <b>OLS output will be the x-value of the datapoint</b>. The y-value of the datapoint is the label, which, 
    in our original example, is whether or not an individual has a high income (<b>we can set y = 1 if the data point 
    has a high income, and y = 0 otherwise</b>). For our dataset, we get the regression line of approximately<br/><br/><span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mo>−</mo><mn>1</mn><mn>6</mn><mi mathvariant="normal">.</mi><mn>4</mn><mo>+</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>2</mn><mn>5</mn><mo>∗</mo><mi>A</mi><mi>g</mi><mi>e</mi><mo>+</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn><mn>3</mn><mo>∗</mo><mi>Y</mi><mi>o</mi><mi>E</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y = -16.4 + 0.25*Age + 0.53*YoE,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.22222em;">Y</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">6</span><span class="mord">.</span><span class="mord">4</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit">A</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">e</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">3</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.22222em;">Y</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mpunct">,</span></span></span></span> </span></span><br/><br/>
    where YoE is short for the “Years of Education” value. We can then plot the points as such: 
    <div><iframe src= https://www.desmos.com/calculator/k60y3zlqsc?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></p><p><b>Notice how the points with X&lt;0 are usually labeled with 0 (low income), while points with X&gt;0 have a label of 1 (high income)</b>.
    We can see that there is a point with X ~ -3.593 that is a clear outlier. Looking back at our exploratory data analysis, 
    this is the outlier point point of (9, 32) that is labeled as high income despite the fact that all 
    similar points are low income. </p></div><div class="idyll-step "><p>
    Moving beyond the dataset, the next question is naturally <b>how would we figure out what our model would label points we haven’t seen before</b>? </p><p>To do so, we fit a sigmoid function on our linear regression line. The sigmoid function is defined as: 
      <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x) =\frac{1}{1+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.845108em;"></span><span class="strut bottom" style="height:1.2484389999999999em;vertical-align:-0.403331em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathit mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7026642857142857em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathit mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.403331em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> </span></span> 
    The sigmoid function is plotted on the right.</p><p><b>The sigmoid function applied to a linear regression model provides the probability that we classify the output 
    as a 1 (having high income) given the linear regression output </b>, or 
    <span style="display:inline-block"><span> <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(y=1 | x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord">1</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span> </span></span>. The mathematical intuition behind 
    this is also beyond the scope of the article, but these probabilities are then used to classify unknown points. We can <b>set a threshold value,
    and then given a probability of the point being labeled as 1, use the threshold to decide how to classify the data point</b>.  
    <br/><br/>
    Changing the threshold could affect what we label points as. Feel free to use the slider below to see how big our linear regression output
    needs to be to label the data as “high income” given different threshold values. <b>Note that the red line is the threshold probability, and the 
    shaded green area shows the corresponding values of X that would be classified as “1”, or having high income</b>.
    
    </p><p>0.3 <input type="range" value="0.5" min="0.3" max="0.7" step="0.1"/> 0.7</p><p>Value of threshold: <span class="idyll-display">0.50</span>.</p><div style="display:none"><div><iframe src= https://www.desmos.com/calculator/t3yudcpd0x?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></div><div style="display:none"><div><iframe src= https://www.desmos.com/calculator/5fralpxz5v?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></div><div><div><iframe src= https://www.desmos.com/calculator/mil4gpaykv?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></div><div style="display:none"><div><iframe src= https://www.desmos.com/calculator/0zmi5vkvor?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></div><div style="display:none"><div><iframe src= https://www.desmos.com/calculator/z9ws9omj9b?embed width="100%" height="400px" style="border: 1px solid #ccc" frameborder=0></iframe></div></div></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-10" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><img src="static/images/machine_learning_cartoon.png"/></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><h1 id="model-selection">Model Selection</h1><p>Now that we introduced ourselves to the three classification models, <b>how would we decide which model to use</b>? </p><p>Note that for the earlier parts of this report, we only ran an algorithm on 32 points of data. However, the dataset these points 
    originally came from actually had 32,561 rows of data! If we run the algorithm on all of the rows (using the best hyperparameters for
    all the models), we would receive the following accuracies:</p><div class="ReactTable table "><div class="rt-table" role="grid"><div class="rt-thead -header" style="min-width:300px"><div class="rt-tr" role="row"><div class="rt-th rt-resizable-header -cursor-pointer" role="columnheader" tabindex="-1" style="flex:100 0 auto;width:100px"><div class="rt-resizable-header-content">alg</div><div class="rt-resizer"></div></div><div class="rt-th rt-resizable-header -cursor-pointer" role="columnheader" tabindex="-1" style="flex:100 0 auto;width:100px"><div class="rt-resizable-header-content">train</div><div class="rt-resizer"></div></div><div class="rt-th rt-resizable-header -cursor-pointer" role="columnheader" tabindex="-1" style="flex:100 0 auto;width:100px"><div class="rt-resizable-header-content">test</div><div class="rt-resizer"></div></div></div></div><div class="rt-tbody" style="min-width:300px"><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -odd" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">KNN</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">0.794</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">0.787</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -even" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">Dt</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">0.799</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">0.784</div></div></div><div class="rt-tr-group" role="rowgroup"><div class="rt-tr -odd" role="row"><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">LogReg</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">0.783</div><div class="rt-td" role="gridcell" style="flex:100 0 auto;width:100px">0.778</div></div></div></div></div><div class="-loading"><div class="-loading-inner">Loading...</div></div></div><p><br/> We see that for each algorithm, the <b>training and test accuracies are close to each other, signifying that our models do not exhibit
    overfitting</b>. In addition, the accuracies are actually pretty close between the different algorithms! This means when looking at accuracy,
    the performance of these models are similar. <b>As a future step, one can look at other measures of performance, such as precision and recall, 
    to determine what the best model is</b>. Other factors, such as an individual’s understanding or interpretability of the model, may also be
    considerations when selecting models in the future. 
    
  </p></div></div></div></div><div class="idyll-scroll" id="idyll-scroll-11" style="position:relative"><div class="idyll-scroll-graphic" style="height:100vh;top:0;left:0;right:0;bottom:auto;width:100%;transform:translate3d(0, 0, 0);z-index:-1"><div style="width:0;position:absolute;left:0;right:0;top:50%;transform:translateY(-50%)"><div class="d3-component-container"><img src="static/images/robot_graduated.png"/></div></div></div><div class=" idyll-text-container"><div class="idyll-scroll-text"><div class="idyll-step "><h1 id="conclusion">Conclusion</h1><p class="introSubtitle">Great Job!</p><p>We hope that you now have a good understanding of the basics of 3 different types of machine learning classification algorithms.  
    As you saw, these algorithms learn from the data input given and use it to classify new observations. 
    These algorithms are used to perform analytical tasks that would take humans hundreds of more hours to perform!  </p><p>Machine learning has applications in many fields like medical, defense, finance, and technology just to name a few.  
    And, classification algorithms are at the heart of many innovations like image recognition, speech recognition, self-driving cars, 
    email spam filtering, and ecommerce product recommendations.  We hope you are inspired to further your knowledge about these topics!</p></div></div></div></div></div></div></div>
    <script src="static/idyll_index.js"></script>
  </body>
</html>
